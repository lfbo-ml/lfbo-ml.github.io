<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="D2C is a VAE-based generative model suitable for few-shot conditional generation.">
  <meta name="keywords" content="VAE, diffusion, generative modeling, contrastive representation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A General Recipe for Likelihood-free Bayesian Optimization</title>

  <script>
    var doc = document.querySelector('link[rel="import"]').import;
    // Grab DOM from doc.html's document.
    var text = doc.querySelector('.doc');
    document.body.appendChild(text.cloneNode(true));
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    th,
    td {
      text-align: left;
      vertical-align: middle;
      border-collapse: collapse;
      border: 0.5px solid black;
      padding: 8px;
    }
  </style>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-four-fifths">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">A General Recipe for Likelihood-free Bayesian Optimization</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://tsong.me">Jiaming Song*</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://lantaoyu.com/">Lantao Yu*</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://willieneis.github.io/">Willie Neiswanger</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://cs.stanford.edu/~ermon/">Stefano Ermon</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>NVIDIA</span>,
              <span class="author-block"><sup>2</sup>Stanford University</span>
              <span class="author-block">*Equal contribution</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                <a href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2106.06819" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/lfbo-ml/lfbo" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://github.com/jiamings/d2c" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code [HyperBandster]</span>
                  </a>
                </span> -->
              </div>
            </div>
            <div class="publication-authors">
              <a href="https://icml.cc/Conferences/2022">
                International Conference on Machine Learning (ICML) 2022
              </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <img id="teaser" src="./static/images/cond-gen.png" height="100%"> -->
  <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video>
        <h2 class="subtitle has-text-centered">
          In Bayesian Optimization (BO), the typical acquisition function requires a probabilistic surrogate model
          (such as Gaussian Processes (GP)). In Likelihood-free Bayesian Optimization (LFBO), the surrogate model is a
          deterministic model that directly reflects the acquisition function.
        </h2>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-5">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The acquisition function, a critical component in Bayesian optimization (BO), can often be written as the
              expectation of a utility function under a surrogate model. However, to ensure that acquisition functions
              are tractable to optimize, restrictions must be placed on the surrogate model and utility function. To
              extend BO to a broader class of models and utilities, we propose likelihood-free BO (LFBO), an approach
              based on likelihood-free inference. </p>
            <p> LFBO directly models the acquisition function without having to
              separately perform inference with a probabilistic surrogate model.
              We show that computing the acquisition function in LFBO can be reduced to optimizing a weighted
              classification problem, where the weights correspond to the utility being chosen.
              LFBO outperforms various state-of-the-art
              black-box optimization methods on several real-world optimization problems.
              LFBO can also effectively leverage composite structures of the objective function, which further improves
              its regret by several orders of magnitude.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">Acquisition functions as expected utility</h2>
            Suppose we have an expensive and possibly-noisy black-box function \(f\), i.e., we can query at a location
            \(x\) and get \(y = f(x)\) but we do not have additional information about \(f\).

            <h2 class="subtitle has-text-centered">
              <img src="./static/images/black_box_f.svg" width="80%">
            </h2>
            <p>
              In Bayesian Optimization, one aims to maximize a black-box function \(f\) from a series of queries. Denote
              the
              \(n\) queries we get as \(\mathcal{D}_n = \{(x_1, y_1), \ldots, (x_n, y_n)\}\). Often, a <b>surrogate
                model</b> \(p(y | x, \mathcal{D}_n)\) is used to decide where we query next (i.e., \(x_{n+1}\)).
              This is done by finding the argmax of an <b>acquisition function</b> in the form of the expectation of
              some utility
              function:
              $$
              \mathbb{E}_{y \sim p(y | x, \mathcal{D}_n)}[u(y; \tau)] = \int u(y; \tau) p(y | x, \mathcal{D}_n)
              \mathrm{d} y
              $$
              where \(\tau\) can be treated as a threshold. Common acquisition functions include:
            <ul>
              <li> <b>Probability of Improvement</b> (PI): \(u^{\mathrm{PI}}(y; \tau) := \mathbb{I}(y - \tau, 0)\),
                where
                \(\mathbb{I}\) is the binary indicator function. This indicates whether \(y\) is above or below the
                threshold \(\tau\) or not.
              </li>
              <li> <b>Expected Improvement</b> (EI): \(u^{\mathrm{EI}}(y; \tau) := \max(y - \tau, 0)\). This indicates
                by how much \(y\) is above the threshold \(\tau\) (if it is below, then 0 is returned).
              </li>
            </ul>
            </p>
            <p>
              When the surrogate model is a GP, the expectations in PI and EI have analytical forms; however, this is
              not necessarily true for other utility functions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">Bayesian Optimization from Classification</h2>
            <p>
              While GPs are very flexible probabilistic models, they can be relatively slow when the number of queries
              is large, or lack representation capacity due to the underlying kernel function; these limitations could
              pose a problem when the search space is very high-dimenisonal.
              <!--(give an example)-->
              Instead, there can be relatively simple methods that can still produce good solutions, but without having
              to use probabilistic models.
            </p>
            <p>
              For example, suppose that we have queried these initial points of the underlying function \(f\) (see
              bottom left). Among the points, some have high \(f(x)\) and others do not; we can draw a line here (\(y =
              \tau\)) as a decision boundary, where for every new point \(x\) we want to predict the probability of
              \(f(x)\) being over the line. We can then train a classifier over the observations to perform this task
              (see bottom right).
            </p>

            <h2 class="subtitle has-text-centered">
              <img src="./static/images/bore_4.png" width="100%">
            </h2>
            <p>
              We can see that this classifier predicts high values closer to the point where the observed \(f(x)\) is
              high. In fact, we can use the classifier as the acquisition function to guide us where to query the next
              point; it turns out to be quite reasonable, as it is close to where we have observed a high \(f(x)\).
            </p>
            <h2 class="subtitle has-text-centered">
              <img src="./static/images/bore_5.png" width="100%">
            </h2>
            <p>
              We can further repeat this process and observe that we have managed to find a pretty reasonable \(f(x)\)
              in a couple of iterations.
            </p>

            <p><br />
            </p>
            <div class="columns is-vcentered interpolation-panel">
              <div class="column is-1">
              </div>
              <div class="column interpolation-video-column">
                <div id="interpolation-image-wrapper">
                  Loading...
                </div>
                <input class="slider is-fullwidth is-large is-info" id="interpolation-slider" step="1" min="0" max="40"
                  value="0" type="range">
                <div id="interpolation-number" class="has-text-centered">
                </div>
              </div>
              <div class="column is-1">
              </div>
            </div>

            <p>
              If we define any distribution over \(x\), then this approach can be treated as trying to estimate the
              density ratio between \(p(x | y > \tau)\) and \(p(x | y < \tau)\); if this ratio for a particular \(x\) is
                high, then we believe that it would have a high \(f(x)\). This approach is therefore
                called "Bayesian Optimization with Ratio Estimation" (BORE), which uses a likelihood-free inference
                method to obtain the density ratio. </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">General acquisition functions from likelihood-free inference</h2>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-two-fifths">
          <div class="content">
            <p>
              The problem with the above approach is that it only considers whether \(y\) is over the threshold
              \(\tau\), but not <b>by how much</b> it is over \(\tau\).
              Therefore, a query that is over the threshold by a massive amount is treated equally as one that is barely
              over it.
            </p>
            <p>
              Can the classifier also take into account information that is beyond the binary indicator (over
              the threshold or not)?
            </p>
            <p>
              In this paper, we propose a simple solution.
            <ul>
              <li>
                In the earlier case, the classifier is trained by treating
                <b>every observation with equal weights</b>.
              </li>
              <li>
                We propose to instead reweight the "positive" queries (i.e., ones that are above the threshold) by its utility value \(u(y, \tau)\).
                When we use the utility associated with the EI acquisition function, we will reweight the queries
                \((x_i, y_i)\) by \(\max(y_i - \tau, 0)\).
              </li>
              <li>
                Here, we place larger weights to queries with larger observation value, which is reasonable because the
                maxima is more likely to yield in their vincinity.
                If we have infinite data and modeling capacity, our learned classifier will converge to EI, unlike the
                one
                used in BORE (which will converge to PI).
              </li>
            </ul>
            </p>
          </div>
        </div>
        <div class="column is-two-fifths">
          <img src="./static/images/weights.png" width="100%">
          <p>
            The new weights are applied only to points that are over the threshold. 
          </p>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>
              After we obtain the weights, the training loss is just standard weighted logistic regression. In PyTorch,
              this is simply written as:
            </p>
            <pre><code>loss = nn.BCEWithLogitsLoss(weight=weight)(target, label)</code></pre>
            <p>The reweighting can also be easily implemented in other standard machine learning frameworks like
              scikit-learn or XGBoost. Beyond the utility associated with the EI acquisition function, we can also use
              any non-negative utility function, which could be intractable with GPs.</p>
            <p>
              As our method is able to learn an acquisition function for any non-negative utility via likelihood-free
              inference, we name our method Likelihood-Free Bayesian Optimization (LFBO). Like BORE, we use
              likelihood-free inference, but unlike it, we are able to learn a general expectation-based acquisition
              functions (instead of just density ratios).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">Comparing different types of acquisition functions</h2>
            <div class="columns is-centered">
              <div class="column">
                <img src="./static/images/lfbo-table.png" width="100%">
                <p>
                <ul>
                  <li> <span style="color:#91751e;"><b>Top</b></span>: Closed-form acquisition functions in the form of
                    expected utility often require
                    certain utility functions (e.g., PI and EI) and certain surrogate models (e.g., GPs).
                  </li>
                  <li><span style="color:#884113;"><b>Middle</b></span>: BORE uses simple, deterministic models (e.g.,
                    neural networks, decision trees) to model a
                    density ratio with likelihood-free inference, but this limits its utility function to the one for
                    PI.
                  </li>
                  <li><span style="color:#506c3d;"><b>Bottom</b></span>: Our likelihood-free BO approach extends BORE to
                    any
                    non-negative utility function, including
                    the one associated with EI and others whose
                    expectations may not have a closed-form solution (e.g., EI with composite function).</li>
                </ul>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">Experimental results on real-world benchmarks</h2>
            <p>
              We compare our approach with a comprehensive set of state-of-the-art black-box optimization methods,
              including BORE, Tree-structured Parzen Estimator (TPE), Sequential Model-based Algorithm Configuration
              (SMAC), GP-based Expected Improvement and Random Search. Here, we use the utility function for EI in LFBO.
              We also conisder three types of models for BORE and LFBO: Multi-Layer Perceptrons (MLP), Gradient-boosted
              Trees (XGB), and Random Forests (RF).
              We report the immediate regret (i.e., the absolute error between the global minimum of each benchmark and
              the lowest function value observed so far) following previous work<a class="footnote">&sup1<span>We note
                  that in this work and BORE, the actual output from the black-box function is a random value from
                  several possible ones (simulating noise in the optimization process), whereas the global minimum
                  considers the expectation over the values. Therefore, a negative immediate regret is possible (and we
                  would use the corresponding neural network for deployment). The flat vertical lines in our plots mean
                  that the average immediate regret (across all the seeds) has become negative.</span></a>.
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h3 class="title is-6">Neural network tuning</h3>
            <p>
              In HPOBench, we aim to find the optimal hyperparameter configurations for training a two-layer
              feed-forward neural networks on four popular UCI datasets. The configuration space includes batch size,
              initial learning rate, learning rate schedule, as well as layer-specific dropout rates, number of units
              and activation functions. For each dataset, HPOBench tabulates the mean squared errors of all 62,208
              configurations.
            </p>
            <img src="./static/images/hpobench.png" width="100%">
            <h3 class="title is-6">Neural architecture search</h3>
            <p>
              In NAS-Bench-201, the network architecture is constructed as a stack of neural cells, each represented as
              a directed acyclic graph (DAG), and we focus on the design of the neural cell, \textit{i.e.}, assigning an
              operation to 6 edges of the DAG from a set of 5 operations. Similar to HPOBench, this dataset also
              provides a lookup table that tabulates the training results of all 15,625 possible configurations for
              three datasets.
            </p>
            <img src="./static/images/nasbench.png" width="100%">
            <p>We can see that our LFBO approach consistently achieves the best result across all the datasets, except
              on NAS-Bench-201-CIFAR-10 where the SMAC method is slightly better. Notably, with all three classifier
              implementations (MLP, RF and XGBoost), LFBO (EI) consistently outperforms its BORE counterparts except for
              Naval with MLP, which demonstrates the effectiveness of choosing EI as the acquisition function and the
              value of enabling EI in the likelihood-free scenario.</p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-5">Experimental results on composite objective functions</h2>
            <p>
              Here, we consider the case of composite objective functions, where \(f(x) = -\Vert h(x) - z_\star
              \Vert_2^2\) is based on a vector-valued black-box function \(h(x)\) and \(z_\star \in \mathbb{R}^d$\) is a
              vector observed from the real world. Compared to standard BO approaches that do not exploit this
              information, a grey-box BO method that explicitly models \(h\) can make a much better sampling decision.
              In LFBO, we can easily take into account the structure of the problem in the classifier model (which we
              call "Composite LFBO"). Composite LFBO approach performs significantly better than regular GP and LFBO,
              while being on par with a composite GP approach.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/comp_bo.png" width="70%">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-5">BibTeX</h2>
          <pre><code>@article{song2022a
  author    = {Song, Jiaming and Yu, Lantao and Neiswanger, Willie and Ermon, Stefano},
  title     = {A General Recipe for Likelihood-free Bayesian Optimization},
  journal   = {arXiv preprint arXiv:2106.06819},
  year      = {2022},
}</code></pre>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>The format of the website is borrowed from the <a href="https://nerfies.github.io/">Nerfies</a> project
              website. <br />
              The following is copied from their website verbatim:</p>
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>